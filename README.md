# Image Captioning using CNN & RNN
🖼️ Image Captioning using CNN & RNN (VGG16 + LSTM) This project demonstrates an end-to-end Image Captioning system using a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). The goal is to generate descriptive captions for images by leveraging both visual and textual data.  📂 Dataset Flickr8k Dataset: Contains 8,000 images, each annotated with five different captions.  We split the dataset into:  90% for training  10% for testing  🧠 Model Architecture Encoder – CNN (VGG16):  A pretrained VGG16 model is used to extract deep visual features from the input images.  The fully connected layers are removed, and the output from the final convolutional layer is used as a feature vector.  Decoder – RNN (LSTM):  An LSTM network generates captions based on the extracted image features.  The model uses an embedding layer to convert words into vector form.  The image feature vector is used as the initial input to the LSTM, followed by word sequences.  🔧 Features Preprocessing of captions (tokenization, padding, vocabulary creation)  Feature extraction using VGG16  Caption generation using greedy decoding  BLEU score evaluation for performance  📊 Results Trained using the Flickr8k dataset with 90/10 train-test split  Model successfully generates meaningful captions for unseen images
